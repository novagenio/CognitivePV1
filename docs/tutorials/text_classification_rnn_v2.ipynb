{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SHRwRoP2nVHX",
        "outputId": "392967f0-4d7e-48de-cd10-da9769ac64fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataset = pd.read_csv('latest_ticket_data.csv')\n",
        "dataset.head()\n"
      ],
      "metadata": {
        "id": "y5DRZjyrB3U2",
        "outputId": "759e788a-733e-4cd0-9bd1-7f8180a2b240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'train'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f62587855920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latest_ticket_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vd4_BGKyurao",
        "outputId": "3479234d-7d45-4ab0-999e-769440d889ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "label:  0\n",
            "train: 0\n",
            "text:  b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'\n",
            "label:  0\n",
            "train: 0\n",
            "text:  b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.'\n",
            "label:  0\n",
            "train: 0\n",
            "text:  b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.'\n",
            "label:  1\n",
            "train: 1\n",
            "text:  b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'\n",
            "label:  1\n",
            "train: 1\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(5):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())\n",
        "  print('train:', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jqkvdcFv41wC",
        "outputId": "db713bed-c5fa-4e63-c939-6a95a0b67334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "texts:  [b'This film is a flagrant rip-off of one of the best novels of all time, Silas Marner by George Eliot.<br /><br />The details of the film shown on IMDb do give acknowledgement to the original authoress but I did not see this at the beginning of the film, only a credit at the end of it saying \"suggested by the book Silas Marner\". Suggested? It was nothing but a complete rip- off of all the essential elements of the story:<br /><br />A wronged and sad old man, an artisan, poor and lonely, has all his money stolen. One night a child wanders up to his door as her mother lies dying in the snow outside. The man takes her in and brings her up until one day the local squire (or rich politician here) demands to adopt the child. It is he who has fathered the child during an illicit affair years before. The battle then ensues as to who should have legal custody of the child.<br /><br />In this and every other aspect of the film, the story is exactly the same. In only one can I find a difference. Silas Marner had epilepsy - but perhaps that would have strained the acting abilities of Mr Martin too far. On top of that he has his hair dyed in some carrot juice concoction (presumably to make him look younger, but actually making him look more the clown that he is)! There is also the addition of meaningless jokes, that this offbeat comedian cannot resist bringing into the story which have no part in it and only detract from the profoundness of the story. Like when the child cries in the courthouse declaring she can only be happy with the man who has fathered her all these years. This is conveyed in the film by the girl applying nasal decongestant to the bridge of her nose to make her tearful!<br /><br />I am surprised that legalities and integrity within the film industry permit such a film to be made. If I was a trustee of George Eliot\\'s I would insist on reparation. If I was Steve Martin I would send the profits to that estate, or to the poor. At the very least it should be entitled Silas Marner - adapted by S Martin. Or better still removed from the archives!<br /><br />If you are interested in this story - and I hope you are - dismiss this completely and watch Silas Marner. Or read the book! The BBC made an excellent adaptation of it in the 1980\\'s.'\n",
            " b'Master director Ching Siu Tung\\'s perhaps most popular achievement is this series, A Chinese Ghost Story 1-3. Chinese Ghost Story stars Leslie Cheung in some distant past in China as a tax collector who is forced to spend a night during his \"collecting trip\" in a mysterious castle in which some strange old warriors fight and meet him. Beautiful actress Joey Wang/Wong is the ghost who lives in that castle and is under a domination of one powerful demon, a wood devil who collects human souls for herself/itself with the help of her beautiful ghosts. Leslie and Joey fall in love, and even though ghosts are not allowed to live with humans, they decide to break that rule and live happily together for the rest of their lives. This is not what the wood devil thinks and our protagonists have to fight for their lives and their happiness.<br /><br />This film is no less full of magic than other films by Ching Siu Tung. His masterpieces include Duel to the Death (1983) and the Swordsman series, which all have incredible visuals and kinetic power in their action scenes. Ghost Story is full of brilliant lightning and dark atmosphere, which is lightened by the strong presence of the beautiful and good willing ghost. The effects are simply breath taking and would work at their greatest power in the big screen. The camera is moving and twisted all the time and it adds to the fairy tale atmosphere this film has. There\\'s plenty of wire\\'fu stunts, too, and even though some think they are and look gratuitous or stupid when used in films, I cannot agree and think they give motion pictures the kind of magic, freedom and creativeness any other tool could not give. When people fly in these films, it means the films are not just about our world, and they usually depict things larger than life with the power of this larger than life art form.<br /><br />The story about the power of love is pretty touching and warm, but the problem is (again) that the characters are little too shallow and act unexplainably occasionally. Leslie and Joey should have been written with greater care and their characters should be even more warm, deep and genuine in order to give the story a greater power and thus make the film even more noteworthy and important achievement. Also, the message about love and power of it is underlined little too much at one point and it should have been left just to the viewer\\'s mind to be interpreted and found. Another negative point about the dialogue is that it\\'s too plenty and people talk in this film without a reason. That is very irritating and sadly shows the flaws many scriptwriters tend to do when they write their movies. People just talk and talk and it\\'s all there just to make everything as easy to understand as possible and so the film is not too challenging or believable as it has this gratuitous element. Just think about the films of the Japanese film maker Takeshi Kitano; his films have very little dialogue and all there is is all the necessary as he tells his things by other tools of cinema and never talks, or makes other characters talk too much in his movies. This is just the talent the writers should have in order to write greater scripts.<br /><br />Otherwise, Chinese Ghost Story is very beautiful and visually breath taking piece of Eastern cinema, and also the song that is played in the film is very beautiful and hopefully earned some award in the Hong Kong film awards back then. I give Chinese Ghost Story 7/10 and without the flaws mentioned above, this would without a doubt be almost perfect masterpiece of the fantasy genre.'\n",
            " b'It took a long time until I could find the title in a special videothek in Berlin, and I was lucky to find an english version with hollandish undertitles. I think it\\xc2\\xb4s one of the best horrormovies ever. It seems strange for me that some people call this movie a black comedy. I must admit, I wasn\\xc2\\xb4t able to laugh about, when I saw it the first time (and it was the same with the second time!) On the one hand Trelkovski seems so nice and even cute in his shy behaviour, but on the other hand he beats this boy on the playground and there is no explanation for that. But the most weired thing is of course his transformation in Simone Choule and the fact, that he doesn\\xc2\\xb4t know, who he really is. His halluzinations are the most terrifying in this movie. Of course it\\xc2\\xb4s all in his mind, but is it this flat that brings out this female side of him or was it also before he moved in - I think that\\xc2\\xb4s an interesting question. His shizophrenic behaviour is hard to understand and it\\xc2\\xb4s horrible to see his two sides or identities fighting against each other. The result of it is that he cuts his hand first and later jumps out of his/her window. But this terrible cry - does that mean, that all will repeat again and again and again... that his soul is in a cage or something? And these egyptian hieroglyphs and other egyptian stuff ? The fact, that he/she looks like a mummy in the Hospital - that\\xc2\\xb4s not an incident, but a clue in my point of view.']\n",
            "\n",
            "labels:  [0 1 1]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tBoyjjWg0Ac9",
        "outputId": "7593502f-aa3e-4caa-c13b-639c59d72388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RGc7C9WiwRWs",
        "outputId": "d78a0bca-a03d-40f5-a3ba-3a233e0faec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 11,  20,   7, ...,   0,   0,   0],\n",
              "       [  1, 172,   1, ...,   2, 996, 533],\n",
              "       [  9, 540,   4, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N_tD0QY5wXaK",
        "outputId": "05802762-157f-46e9-96f6-8b5d15708e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  b'This film is a flagrant rip-off of one of the best novels of all time, Silas Marner by George Eliot.<br /><br />The details of the film shown on IMDb do give acknowledgement to the original authoress but I did not see this at the beginning of the film, only a credit at the end of it saying \"suggested by the book Silas Marner\". Suggested? It was nothing but a complete rip- off of all the essential elements of the story:<br /><br />A wronged and sad old man, an artisan, poor and lonely, has all his money stolen. One night a child wanders up to his door as her mother lies dying in the snow outside. The man takes her in and brings her up until one day the local squire (or rich politician here) demands to adopt the child. It is he who has fathered the child during an illicit affair years before. The battle then ensues as to who should have legal custody of the child.<br /><br />In this and every other aspect of the film, the story is exactly the same. In only one can I find a difference. Silas Marner had epilepsy - but perhaps that would have strained the acting abilities of Mr Martin too far. On top of that he has his hair dyed in some carrot juice concoction (presumably to make him look younger, but actually making him look more the clown that he is)! There is also the addition of meaningless jokes, that this offbeat comedian cannot resist bringing into the story which have no part in it and only detract from the profoundness of the story. Like when the child cries in the courthouse declaring she can only be happy with the man who has fathered her all these years. This is conveyed in the film by the girl applying nasal decongestant to the bridge of her nose to make her tearful!<br /><br />I am surprised that legalities and integrity within the film industry permit such a film to be made. If I was a trustee of George Eliot\\'s I would insist on reparation. If I was Steve Martin I would send the profits to that estate, or to the poor. At the very least it should be entitled Silas Marner - adapted by S Martin. Or better still removed from the archives!<br /><br />If you are interested in this story - and I hope you are - dismiss this completely and watch Silas Marner. Or read the book! The BBC made an excellent adaptation of it in the 1980\\'s.'\n",
            "Round-trip:  this film is a [UNK] [UNK] of one of the best [UNK] of all time [UNK] [UNK] by george [UNK] br the [UNK] of the film shown on imdb do give [UNK] to the original [UNK] but i did not see this at the beginning of the film only a [UNK] at the end of it saying [UNK] by the book [UNK] [UNK] [UNK] it was nothing but a complete [UNK] off of all the [UNK] elements of the [UNK] br a [UNK] and sad old man an [UNK] poor and [UNK] has all his money [UNK] one night a child [UNK] up to his [UNK] as her mother [UNK] [UNK] in the [UNK] outside the man takes her in and brings her up until one day the local [UNK] or [UNK] [UNK] here [UNK] to [UNK] the child it is he who has [UNK] the child during an [UNK] [UNK] years before the battle then [UNK] as to who should have [UNK] [UNK] of the [UNK] br in this and every other [UNK] of the film the story is exactly the same in only one can i find a [UNK] [UNK] [UNK] had [UNK] but perhaps that would have [UNK] the acting [UNK] of mr [UNK] too far on top of that he has his [UNK] [UNK] in some [UNK] [UNK] [UNK] [UNK] to make him look [UNK] but actually making him look more the [UNK] that he is there is also the [UNK] of [UNK] jokes that this [UNK] [UNK] cannot [UNK] [UNK] into the story which have no part in it and only [UNK] from the [UNK] of the story like when the child [UNK] in the [UNK] [UNK] she can only be happy with the man who has [UNK] her all these years this is [UNK] in the film by the girl [UNK] [UNK] [UNK] to the [UNK] of her [UNK] to make her [UNK] br i am surprised that [UNK] and [UNK] within the film [UNK] [UNK] such a film to be made if i was a [UNK] of george [UNK] i would [UNK] on [UNK] if i was [UNK] [UNK] i would [UNK] the [UNK] to that [UNK] or to the poor at the very least it should be [UNK] [UNK] [UNK] [UNK] by [UNK] [UNK] or better still [UNK] from the [UNK] br if you are interested in this story and i hope you are [UNK] this completely and watch [UNK] [UNK] or read the book the [UNK] made an excellent [UNK] of it in the [UNK]                                                                                                                                                                                                                   \n",
            "\n",
            "Original:  b'Master director Ching Siu Tung\\'s perhaps most popular achievement is this series, A Chinese Ghost Story 1-3. Chinese Ghost Story stars Leslie Cheung in some distant past in China as a tax collector who is forced to spend a night during his \"collecting trip\" in a mysterious castle in which some strange old warriors fight and meet him. Beautiful actress Joey Wang/Wong is the ghost who lives in that castle and is under a domination of one powerful demon, a wood devil who collects human souls for herself/itself with the help of her beautiful ghosts. Leslie and Joey fall in love, and even though ghosts are not allowed to live with humans, they decide to break that rule and live happily together for the rest of their lives. This is not what the wood devil thinks and our protagonists have to fight for their lives and their happiness.<br /><br />This film is no less full of magic than other films by Ching Siu Tung. His masterpieces include Duel to the Death (1983) and the Swordsman series, which all have incredible visuals and kinetic power in their action scenes. Ghost Story is full of brilliant lightning and dark atmosphere, which is lightened by the strong presence of the beautiful and good willing ghost. The effects are simply breath taking and would work at their greatest power in the big screen. The camera is moving and twisted all the time and it adds to the fairy tale atmosphere this film has. There\\'s plenty of wire\\'fu stunts, too, and even though some think they are and look gratuitous or stupid when used in films, I cannot agree and think they give motion pictures the kind of magic, freedom and creativeness any other tool could not give. When people fly in these films, it means the films are not just about our world, and they usually depict things larger than life with the power of this larger than life art form.<br /><br />The story about the power of love is pretty touching and warm, but the problem is (again) that the characters are little too shallow and act unexplainably occasionally. Leslie and Joey should have been written with greater care and their characters should be even more warm, deep and genuine in order to give the story a greater power and thus make the film even more noteworthy and important achievement. Also, the message about love and power of it is underlined little too much at one point and it should have been left just to the viewer\\'s mind to be interpreted and found. Another negative point about the dialogue is that it\\'s too plenty and people talk in this film without a reason. That is very irritating and sadly shows the flaws many scriptwriters tend to do when they write their movies. People just talk and talk and it\\'s all there just to make everything as easy to understand as possible and so the film is not too challenging or believable as it has this gratuitous element. Just think about the films of the Japanese film maker Takeshi Kitano; his films have very little dialogue and all there is is all the necessary as he tells his things by other tools of cinema and never talks, or makes other characters talk too much in his movies. This is just the talent the writers should have in order to write greater scripts.<br /><br />Otherwise, Chinese Ghost Story is very beautiful and visually breath taking piece of Eastern cinema, and also the song that is played in the film is very beautiful and hopefully earned some award in the Hong Kong film awards back then. I give Chinese Ghost Story 7/10 and without the flaws mentioned above, this would without a doubt be almost perfect masterpiece of the fantasy genre.'\n",
            "Round-trip:  [UNK] director [UNK] [UNK] [UNK] perhaps most [UNK] [UNK] is this series a [UNK] [UNK] story [UNK] [UNK] [UNK] story stars [UNK] [UNK] in some [UNK] past in [UNK] as a [UNK] [UNK] who is forced to [UNK] a night during his [UNK] [UNK] in a [UNK] [UNK] in which some strange old [UNK] fight and meet him beautiful actress [UNK] [UNK] is the [UNK] who lives in that [UNK] and is under a [UNK] of one powerful [UNK] a [UNK] [UNK] who [UNK] human [UNK] for [UNK] with the help of her beautiful [UNK] [UNK] and [UNK] fall in love and even though [UNK] are not [UNK] to live with [UNK] they [UNK] to [UNK] that [UNK] and live [UNK] together for the rest of their lives this is not what the [UNK] [UNK] [UNK] and our [UNK] have to fight for their lives and their [UNK] br this film is no less full of [UNK] than other films by [UNK] [UNK] [UNK] his [UNK] [UNK] [UNK] to the death [UNK] and the [UNK] series which all have [UNK] [UNK] and [UNK] power in their action scenes [UNK] story is full of brilliant [UNK] and dark atmosphere which is [UNK] by the strong [UNK] of the beautiful and good [UNK] [UNK] the effects are simply [UNK] taking and would work at their greatest power in the big screen the camera is moving and [UNK] all the time and it [UNK] to the [UNK] tale atmosphere this film has theres plenty of [UNK] [UNK] too and even though some think they are and look [UNK] or stupid when used in films i cannot [UNK] and think they give [UNK] [UNK] the kind of [UNK] [UNK] and [UNK] any other [UNK] could not give when people [UNK] in these films it means the films are not just about our world and they usually [UNK] things [UNK] than life with the power of this [UNK] than life art [UNK] br the story about the power of love is pretty [UNK] and [UNK] but the problem is again that the characters are little too [UNK] and act [UNK] [UNK] [UNK] and [UNK] should have been written with [UNK] care and their characters should be even more [UNK] deep and [UNK] in order to give the story a [UNK] power and [UNK] make the film even more [UNK] and important [UNK] also the message about love and power of it is [UNK] little too much at one point and it should have been left just to the viewers mind to be [UNK] and found another [UNK] point about the dialogue is that its too plenty and people talk in this film without a reason that is very [UNK] and [UNK] shows the [UNK] many [UNK] [UNK] to do when they write their movies people just talk and talk and its all there just to make everything as easy to understand as possible and so the film is not too [UNK] or believable as it has this [UNK] [UNK] just think about the films of the japanese film [UNK] [UNK] [UNK] his films have very little dialogue and all there is is all the [UNK] as he tells his things by other [UNK] of cinema and never [UNK] or makes other characters talk too much in his movies this is just the talent the writers should have in order to write [UNK] [UNK] br otherwise [UNK] [UNK] story is very beautiful and [UNK] [UNK] taking piece of [UNK] cinema and also the song that is played in the film is very beautiful and [UNK] [UNK] some [UNK] in the [UNK] [UNK] film [UNK] back then i give [UNK] [UNK] story [UNK] and without the [UNK] [UNK] above this would without a doubt be almost perfect [UNK] of the fantasy genre\n",
            "\n",
            "Original:  b'It took a long time until I could find the title in a special videothek in Berlin, and I was lucky to find an english version with hollandish undertitles. I think it\\xc2\\xb4s one of the best horrormovies ever. It seems strange for me that some people call this movie a black comedy. I must admit, I wasn\\xc2\\xb4t able to laugh about, when I saw it the first time (and it was the same with the second time!) On the one hand Trelkovski seems so nice and even cute in his shy behaviour, but on the other hand he beats this boy on the playground and there is no explanation for that. But the most weired thing is of course his transformation in Simone Choule and the fact, that he doesn\\xc2\\xb4t know, who he really is. His halluzinations are the most terrifying in this movie. Of course it\\xc2\\xb4s all in his mind, but is it this flat that brings out this female side of him or was it also before he moved in - I think that\\xc2\\xb4s an interesting question. His shizophrenic behaviour is hard to understand and it\\xc2\\xb4s horrible to see his two sides or identities fighting against each other. The result of it is that he cuts his hand first and later jumps out of his/her window. But this terrible cry - does that mean, that all will repeat again and again and again... that his soul is in a cage or something? And these egyptian hieroglyphs and other egyptian stuff ? The fact, that he/she looks like a mummy in the Hospital - that\\xc2\\xb4s not an incident, but a clue in my point of view.'\n",
            "Round-trip:  it took a long time until i could find the title in a special [UNK] in [UNK] and i was [UNK] to find an english version with [UNK] [UNK] i think [UNK] one of the best [UNK] ever it seems strange for me that some people call this movie a black comedy i must admit i [UNK] able to laugh about when i saw it the first time and it was the same with the second time on the one hand [UNK] seems so nice and even [UNK] in his [UNK] [UNK] but on the other hand he [UNK] this boy on the [UNK] and there is no [UNK] for that but the most [UNK] thing is of course his [UNK] in [UNK] [UNK] and the fact that he [UNK] know who he really is his [UNK] are the most [UNK] in this movie of course [UNK] all in his mind but is it this [UNK] that brings out this female side of him or was it also before he [UNK] in i think [UNK] an interesting question his [UNK] [UNK] is hard to understand and [UNK] horrible to see his two [UNK] or [UNK] fighting against each other the result of it is that he [UNK] his hand first and later [UNK] out of [UNK] [UNK] but this terrible [UNK] does that mean that all will [UNK] again and again and again that his [UNK] is in a [UNK] or something and these [UNK] [UNK] and other [UNK] stuff the fact that [UNK] looks like a [UNK] in the [UNK] [UNK] not an [UNK] but a [UNK] in my point of view                                                                                                                                                                                                                                                                                                                                                                          \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "87a8-CwfKebw",
        "outputId": "5a61bcd2-8535-4cb2-fb7a-e22a83cb85f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O41gw3KfWHus",
        "outputId": "ae311631-21af-474e-bf33-05a8d3802a0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00749027]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UIgpuTeFNDzq",
        "outputId": "b770e5a0-e594-415a-a964-c0d7e3031884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00749027]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/layered_bidirectional.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvpE3BaGw_V"
      },
      "source": [
        "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
        "\n",
        "If you're interestied in building custom RNNs, see the [Keras RNN Guide](https://www.tensorflow.org/guide/keras/rnn).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_classification_rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}